{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355a5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9d8d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_delta_rule(Q, K, V, beta, chunk_size):\n",
    "    \"\"\"\n",
    "    Applies chunked attention mechanism to the input queries, keys, and values.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "        K: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "        V: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "        chunk_size: Size of each chunk\n",
    "    Returns:\n",
    "        Output tensor after applying chunked attention of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    B, L, d = Q.shape\n",
    "    C = chunk_size\n",
    "    K_beta = K * beta.unsqueeze(-1)  # (B, L, d)\n",
    "    V_beta = V * beta.unsqueeze(-1)  # (B, L, d)\n",
    "    T = - (K_beta @ K.transpose(-2, -1)).tril(-1)  # (B, L, L)\n",
    "    \n",
    "    for i in range(1, C):\n",
    "        # Keep batch dimension: T[:, i, :i] has shape (B, i)\n",
    "        T[:, i, :i] = T[:, i, :i] + (T[:, i, :, None] * T[:, :, :i]).sum(-2)\n",
    "\n",
    "    T = T + torch.eye(L).to(Q.device).unsqueeze(0)  # (B, L, L)\n",
    "    W = T @ K_beta  # (B, L, d)\n",
    "    U = T @ V_beta  # (B, L, d)\n",
    "    S = torch.zeros(B, d, d).to(Q.device)  # (B, d, d)\n",
    "    O = torch.empty_like(V)\n",
    "    \n",
    "    # Reshape to chunks: (B, num_chunks, chunk_size, d)\n",
    "    num_chunks = L // C\n",
    "    Q_chunks = Q.reshape(B, num_chunks, C, d)\n",
    "    K_chunks = K.reshape(B, num_chunks, C, d)\n",
    "    W_chunks = W.reshape(B, num_chunks, C, d)\n",
    "    U_chunks = U.reshape(B, num_chunks, C, d)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        q_i = Q_chunks[:, i]  # (B, C, d)\n",
    "        k_i = K_chunks[:, i]  # (B, C, d)\n",
    "        w_i = W_chunks[:, i]  # (B, C, d)\n",
    "        u_i = U_chunks[:, i] - w_i @ S  # (B, C, d)\n",
    "        o_inter = q_i @ S  # (B, C, d)\n",
    "        A_i = (q_i @ k_i.transpose(-2, -1)).tril()  # (B, C, C)\n",
    "        o_intra = A_i @ u_i  # (B, C, d)\n",
    "        O[:, i*C:(i+1)*C] = o_inter + o_intra\n",
    "        S = S + k_i.transpose(-2, -1) @ u_i  # (B, d, d)\n",
    "    \n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "107de248",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, d = 2, 10, 4\n",
    "Q = torch.randn(B, L, d)\n",
    "K = torch.randn(B, L, d)\n",
    "V = torch.randn(B, L, d)\n",
    "chunk_size = 2\n",
    "beta = torch.ones(B, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ed6ffec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_delta_rule(Q, K, V, beta, chunk_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353d975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
