{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355a5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d8d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_delta_rule(Q, K, V, beta, chunk_size):\n",
    "    \"\"\"\n",
    "    Applies chunked attention mechanism to the input queries, keys, and values.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "        K: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "        V: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "        chunk_size: Size of each chunk\n",
    "    Returns:\n",
    "        Output tensor after applying chunked attention of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    B, L, d = Q.shape\n",
    "    C = chunk_size\n",
    "    K_beta = K * beta.unsqueeze(-1)  # (B, L, d)\n",
    "    V_beta = V * beta.unsqueeze(-1)  # (B, L, d)\n",
    "    T = - (K_beta @ K.transpose(-2, -1)).tril(-1)  # (B, L, L)\n",
    "    \n",
    "    for i in range(1, C):\n",
    "        # Keep batch dimension: T[:, i, :i] has shape (B, i)\n",
    "        T[:, i, :i] = T[:, i, :i] + (T[:, i, :, None] * T[:, :, :i]).sum(-2)\n",
    "\n",
    "    T = T + torch.eye(L).to(Q.device).unsqueeze(0)  # (B, L, L)\n",
    "    W = T @ K_beta  # (B, L, d)\n",
    "    U = T @ V_beta  # (B, L, d)\n",
    "    S = torch.zeros(B, d, d).to(Q.device)  # (B, d, d)\n",
    "    O = torch.empty_like(V)\n",
    "    \n",
    "    # Reshape to chunks: (B, num_chunks, chunk_size, d)\n",
    "    num_chunks = L // C\n",
    "    Q_chunks = Q.reshape(B, num_chunks, C, d)\n",
    "    K_chunks = K.reshape(B, num_chunks, C, d)\n",
    "    W_chunks = W.reshape(B, num_chunks, C, d)\n",
    "    U_chunks = U.reshape(B, num_chunks, C, d)\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        q_i = Q_chunks[:, i]  # (B, C, d)\n",
    "        k_i = K_chunks[:, i]  # (B, C, d)\n",
    "        w_i = W_chunks[:, i]  # (B, C, d)\n",
    "        u_i = U_chunks[:, i] - w_i @ S  # (B, C, d)\n",
    "        o_inter = q_i @ S  # (B, C, d)\n",
    "        A_i = (q_i @ k_i.transpose(-2, -1)).tril()  # (B, C, C)\n",
    "        o_intra = A_i @ u_i  # (B, C, d)\n",
    "        O[:, i*C:(i+1)*C] = o_inter + o_intra\n",
    "        S = S + k_i.transpose(-2, -1) @ u_i  # (B, d, d)\n",
    "    \n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107de248",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L, d = 2, 10, 4\n",
    "Q = torch.randn(B, L, d)\n",
    "K = torch.randn(B, L, d)\n",
    "V = torch.randn(B, L, d)\n",
    "chunk_size = 2\n",
    "beta = torch.ones(B, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed6ffec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_delta_rule(Q, K, V, beta, chunk_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c77a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "        rms = norm_x / (x.size(-1) ** 0.5)\n",
    "        x_normed = x / (rms + self.eps)\n",
    "        return x_normed * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8353d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaNet(nn.Module):\n",
    "    def __init__(self, d_model, chunk_size):\n",
    "        super(DeltaNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.beta_linear = nn.Linear(d_model, 1)\n",
    "        self.q_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.k_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.v_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q_linear(x)\n",
    "        q = nn.functional.relu(self.q_conv(q.transpose(1, 2))).transpose(1, 2)\n",
    "        q = q / (q.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "        k = self.k_linear(x)\n",
    "        k = nn.functional.relu(self.k_conv(k.transpose(1, 2))).transpose(1, 2)\n",
    "        k = k / (k.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "        v = self.v_linear(x)\n",
    "        v = nn.functional.relu(self.v_conv(v.transpose(1, 2))).transpose(1, 2)\n",
    "\n",
    "        beta = torch.sigmoid(self.beta_linear(x)).squeeze(-1)\n",
    "\n",
    "        out = chunk_delta_rule(q, k, v, beta, self.chunk_size)\n",
    "        out = self.rms_norm(out)\n",
    "        out = self.output_linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9f547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SwiGLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return x1 * torch.sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b9ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, chunk_size, ff_hidden_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.deltanet = DeltaNet(d_model, chunk_size)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden_dim * 2),\n",
    "            SwiGLU(),\n",
    "            nn.Linear(ff_hidden_dim, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        deltanet_attn_out = self.deltanet(x)\n",
    "        x = x + deltanet_attn_out\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + ffn_out\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9d579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDeltaNet(nn.Module):\n",
    "    def __init__(self, d_model, chunk_size, num_layers):\n",
    "        super(TransformerDeltaNet, self).__init__()\n",
    "        self.layers = nn.ModuleList([DeltaNet(d_model, chunk_size) for _ in range(num_layers)])\n",
    "        self.final_rms_norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + x  # Residual connection\n",
    "        x = self.final_rms_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerDeltaNet(d_model=64, chunk_size=4, num_layers=6)\n",
    "input_tensor = torch.randn(8, 128, 64)  # (batch_size, seq_len, d_model)\n",
    "output = model(input_tensor)\n",
    "output.shape  # (8, 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5bf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
