{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355a5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363cb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_delta_rule(q, k, v, beta, chunk_size):\n",
    "    \"\"\"\n",
    "    Alternative implementation of chunked attention mechanism to the input queries, keys, and values.\n",
    "\n",
    "    Args:\n",
    "        q: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "        k: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "        v: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "        chunk_size: Size of each chunk\n",
    "    Returns:\n",
    "        Output tensor after applying chunked attention of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\" \n",
    "    B, H, L, D = q.shape\n",
    "    C = chunk_size\n",
    "    num_chunks = L // C\n",
    "\n",
    "    # Reshape inputs for chunk processing: [B, H, num_chunks, C, D]\n",
    "    q = q.reshape(B, H, num_chunks, C, D)\n",
    "    k = k.reshape(B, H, num_chunks, C, D)\n",
    "    v = v.reshape(B, H, num_chunks, C, D)\n",
    "    beta = beta.reshape(B, H, num_chunks, C)\n",
    "\n",
    "    k_beta = k * beta.unsqueeze(-1)  # (B, H, num_chunks, C, D)\n",
    "    v_beta = v * beta.unsqueeze(-1)  # (B, H, num_chunks, C, D)\n",
    "\n",
    "    T = -(k_beta @ k.transpose(-1, -2)).tril(-1)  # (B, H, num_chunks, C, C)\n",
    "    for i in range(1, C):\n",
    "        T[..., i, :i] = T[..., i, :i] + (T[..., i, :, None] * T[..., :, :i]).sum(-2)\n",
    "\n",
    "    T = T + torch.eye(C, device=q.device, dtype=q.dtype)\n",
    "    \n",
    "    W = T @ k_beta  # (B, H, num_chunks, C, D)\n",
    "    U = T @ v_beta  # (B, H, num_chunks, C, D)\n",
    "\n",
    "    S = torch.zeros(B, H, D, D, device=q.device, dtype=q.dtype)\n",
    "    O = torch.empty_like(v)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        q_i = q[:, :, i]  # (B, H, C, D)\n",
    "        k_i = k[:, :, i]  # (B, H, C, D)\n",
    "        w_i = W[:, :, i]  # (B, H, C, D)\n",
    "        u_init = U[:, :, i]  # (B, H, C, D)\n",
    "\n",
    "        u_i = u_init - (w_i @ S)\n",
    "        o_inter = (q_i @ S)            \n",
    "        a_i = (q_i @ k_i.transpose(-1, -2)).tril()\n",
    "        o_intra = a_i @ u_i\n",
    "\n",
    "        O[:, :, i] = o_intra + o_inter\n",
    "        S = S + (k_i.transpose(-1, -2) @ u_i)\n",
    "\n",
    "    return O.reshape(B, H, L, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107de248",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, L, d = 2, 8, 1024, 64\n",
    "Q = torch.randn(B, H, L, d)\n",
    "K = torch.randn(B, H, L, d)\n",
    "V = torch.randn(B, H, L, d)\n",
    "chunk_size = 16\n",
    "beta = torch.ones(B, H, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed6ffec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 1024, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_delta_rule(Q, K, V, beta, chunk_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c77a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "        rms = norm_x / (x.size(-1) ** 0.5)\n",
    "        x_normed = x / (rms + self.eps)\n",
    "        return x_normed * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8353d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaNet(nn.Module):\n",
    "    def __init__(self, d_model, chunk_size=64, num_heads=8):\n",
    "        super(DeltaNet, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.beta_linear = nn.Linear(d_model, num_heads)\n",
    "        \n",
    "        self.q_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.k_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        self.v_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        H = self.num_heads\n",
    "        D = self.head_dim\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.q_linear(x)\n",
    "        q = nn.functional.silu(self.q_conv(q.transpose(1, 2))).transpose(1, 2)\n",
    "        \n",
    "        k = self.k_linear(x)\n",
    "        k = nn.functional.silu(self.k_conv(k.transpose(1, 2))).transpose(1, 2)\n",
    "        \n",
    "        v = self.v_linear(x)\n",
    "        v = nn.functional.silu(self.v_conv(v.transpose(1, 2))).transpose(1, 2)\n",
    "        \n",
    "        # Reshape to multi-head format: (B, L, d_model) -> (B, H, L, head_dim)\n",
    "        q = q.view(B, L, H, D).transpose(1, 2)  # (B, H, L, D)\n",
    "        k = k.view(B, L, H, D).transpose(1, 2)  # (B, H, L, D)\n",
    "        v = v.view(B, L, H, D).transpose(1, 2)  # (B, H, L, D)\n",
    "        \n",
    "        # Normalize q and k per head\n",
    "        q = q / (q.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "        k = k / (k.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Beta: (B, L, num_heads) -> (B, H, L)\n",
    "        beta = torch.sigmoid(self.beta_linear(x))  # (B, L, H)\n",
    "        beta = beta.transpose(1, 2)  # (B, H, L)\n",
    "\n",
    "        # Apply chunk delta rule with multi-head\n",
    "        out = chunk_delta_rule(q, k, v, beta, self.chunk_size)  # (B, H, L, D)\n",
    "        \n",
    "        # Reshape back: (B, H, L, D) -> (B, L, d_model)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
    "        \n",
    "        out = self.rms_norm(out)\n",
    "        out = self.output_linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9f547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, bias=False):\n",
    "        super().__init__()\n",
    "        # SwiGLU: (Swish(xW) * xV)W_o\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=bias) # Gate\n",
    "        self.w2 = nn.Linear(dim, hidden_dim, bias=bias) # Value\n",
    "        self.w3 = nn.Linear(hidden_dim, dim, bias=bias) # Output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.w1(x)\n",
    "        x2 = self.w2(x)\n",
    "        # Swish/SiLU activation\n",
    "        hidden = F.silu(x1) * x2\n",
    "        return self.w3(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b9ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaNetBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        # First residual path: DeltaNet\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.delta_net = DeltaNet(dim, num_heads)\n",
    "        \n",
    "        # Second residual path: SwiGLU\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = SwiGLU(dim, mlp_hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x + DeltaNet(RMSNorm(x))\n",
    "        x = x + self.delta_net(self.norm1(x))\n",
    "        # x + SwiGLU(RMSNorm(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9d579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaNetModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, depth, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        \n",
    "        # N x Blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            DeltaNetBlock(dim, num_heads) \n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final Norm\n",
    "        self.norm_f = RMSNorm(dim)\n",
    "        \n",
    "        # Final Linear (Head)\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Inputs\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # N x Stacked Layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Final Norm + Linear\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26a6921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 1000])\n"
     ]
    }
   ],
   "source": [
    "B, L, D = 2, 64, 128\n",
    "V = 1000\n",
    "heads = 4\n",
    "\n",
    "model = DeltaNetModel(vocab_size=V, dim=D, depth=2, num_heads=heads)\n",
    "input_ids = torch.randint(0, V, (B, L))\n",
    "logits = model(input_ids) # (2, 64, 1000)\n",
    "print(logits.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5bf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
